{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd4a65ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores shape: (6696, 76)\n",
      "First few columns: ['question_ind', 'section', 'tfidf', 'embedding', 'attention_0_0', 'attention_0_1', 'attention_0_2', 'attention_0_3', 'attention_0_4', 'attention_0_5']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "\n",
    "scores_path = r\"C:\\Users\\KVIRDI\\Downloads\\scores.csv\"\n",
    "scores = pd.read_csv(scores_path)\n",
    "\n",
    "print(\"Scores shape:\", scores.shape)\n",
    "print(\"First few columns:\", scores.columns.tolist()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3e5dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_path = r\"C:\\Users\\KVIRDI\\Downloads\\RSM_Questions.docx\"\n",
    "doc = Document(questions_path)\n",
    "\n",
    "questions = pd.DataFrame(columns=['question', 'section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f28ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 54\n",
      "                                            question  section\n",
      "0  What is the non-fixed radiation contamination ...  2.4.6.2\n",
      "1  What procedures should be followed if the non-...  2.4.6.2\n",
      "2  When should the wipe tests be done after using...  2.4.6.1\n",
      "3  When is leak testing of sealed sources or devi...    2.4.8\n",
      "4  What is the schedule for leak testing the seal...    2.4.8\n"
     ]
    }
   ],
   "source": [
    "text = [p.text for p in doc.paragraphs if p.text]\n",
    "\n",
    "for i in range(0, len(text), 2):\n",
    "    question = text[i]\n",
    "    section_line = text[i + 1]\n",
    "\n",
    "    section_comps = section_line.strip(' ').split(' ')\n",
    "\n",
    "    # Same multi-label logic you had before\n",
    "    if len(section_comps) > 2:\n",
    "        if section_comps[0] == 'Appendix':\n",
    "            label = 'Appendix 6,Appendix 7'\n",
    "        else:\n",
    "            label = '1.3.4.1,2.2.1'\n",
    "    else:\n",
    "        label = section_comps[-1]\n",
    "\n",
    "    questions.loc[len(questions), :] = [question, label]\n",
    "\n",
    "print(\"Number of questions:\", len(questions))\n",
    "print(questions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93714e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 108\n",
      "                                            question  section\n",
      "0  What is the non-fixed radiation contamination ...  2.4.6.2\n",
      "1  What procedures should be followed if the non-...  2.4.6.2\n",
      "2  When should the wipe tests be done after using...  2.4.6.1\n",
      "3  When is leak testing of sealed sources or devi...    2.4.8\n",
      "4  What is the schedule for leak testing the seal...    2.4.8\n",
      "                                            question  section gold_sections\n",
      "0  What is the non-fixed radiation contamination ...  2.4.6.2     [2.4.6.2]\n",
      "1  What procedures should be followed if the non-...  2.4.6.2     [2.4.6.2]\n",
      "2  When should the wipe tests be done after using...  2.4.6.1     [2.4.6.1]\n",
      "3  When is leak testing of sealed sources or devi...    2.4.8       [2.4.8]\n",
      "4  What is the schedule for leak testing the seal...    2.4.8       [2.4.8]\n"
     ]
    }
   ],
   "source": [
    "text = [p.text for p in doc.paragraphs if p.text]\n",
    "\n",
    "for i in range(0, len(text), 2):\n",
    "    question = text[i]\n",
    "    section_line = text[i + 1]\n",
    "\n",
    "    section_comps = section_line.strip(' ').split(' ')\n",
    "\n",
    "    if len(section_comps) > 2:\n",
    "        if section_comps[0] == 'Appendix':\n",
    "            label = 'Appendix 6,Appendix 7'\n",
    "        else:\n",
    "            label = '1.3.4.1,2.2.1'\n",
    "    else:\n",
    "        label = section_comps[-1]\n",
    "\n",
    "    questions.loc[len(questions), :] = [question, label]\n",
    "\n",
    "print(\"Number of questions:\", len(questions))\n",
    "print(questions.head())\n",
    "\n",
    "questions[\"gold_sections\"] = questions[\"section\"].apply(\n",
    "    lambda s: [x.strip() for x in str(s).split(\",\")]\n",
    ")\n",
    "\n",
    "print(questions[[\"question\", \"section\", \"gold_sections\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1436d49e",
   "metadata": {},
   "source": [
    "Model Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95279036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top1_accuracy(scores_df, questions_df, method_col):\n",
    "  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    valid_q_indices = sorted(scores_df[\"question_ind\"].unique())\n",
    "\n",
    "    for q_idx in valid_q_indices:\n",
    "        if q_idx >= len(questions_df):\n",
    "            print(f\"Skipping q_idx {q_idx}: not present in questions_df\")\n",
    "            continue\n",
    "\n",
    "        q_scores = scores_df[scores_df[\"question_ind\"] == q_idx]\n",
    "\n",
    "        if q_scores.empty:\n",
    "            print(f\"Warning: no scores for question_ind {q_idx}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        best_row = q_scores.sort_values(method_col, ascending=False).iloc[0]\n",
    "        pred_section = str(best_row[\"section\"]).strip()\n",
    "\n",
    "        gold_list = questions_df.loc[q_idx, \"gold_sections\"]\n",
    "\n",
    "        if pred_section in gold_list:\n",
    "            correct += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "    if total == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    return correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52f7c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved accuracies to: C:\\Users\\KVIRDI\\Downloads\\model_accuracies.csv\n"
     ]
    }
   ],
   "source": [
    "method_cols = [\"tfidf\", \"embedding\"]\n",
    "\n",
    "attention_cols = [c for c in scores.columns if c.startswith(\"attention_\")]\n",
    "method_cols = method_cols + attention_cols\n",
    "\n",
    "results = []\n",
    "for m in method_cols:\n",
    "    acc = top1_accuracy(scores, questions, m)\n",
    "    results.append({\"method\": m, \"top1_accuracy\": acc})\n",
    "\n",
    "accuracy_df = pd.DataFrame(results).sort_values(\"top1_accuracy\", ascending=False)\n",
    "\n",
    "out_path = r\"C:\\Users\\KVIRDI\\Downloads\\model_accuracies.csv\"\n",
    "accuracy_df.to_csv(out_path, index=False)\n",
    "print(f\"\\nSaved accuracies to: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b090d1c",
   "metadata": {},
   "source": [
    "Model Predictions: Using the best attention model on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to: C:\\Users\\KVIRDI\\Downloads\\model_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "def get_top_predictions(scores_df, questions_df, method_cols):\n",
    "  \n",
    "    records = []\n",
    "    valid_q_indices = sorted(scores_df[\"question_ind\"].unique())\n",
    "\n",
    "    for q_idx in valid_q_indices:\n",
    "        if q_idx >= len(questions_df):\n",
    "            continue\n",
    "\n",
    "        gold_list = questions_df.loc[q_idx, \"gold_sections\"]\n",
    "        question_text = questions_df.loc[q_idx, \"question\"]\n",
    "\n",
    "        q_scores = scores_df[scores_df[\"question_ind\"] == q_idx]\n",
    "\n",
    "        for m in method_cols:\n",
    "            if m not in scores_df.columns:\n",
    "                continue\n",
    "\n",
    "            qs = q_scores.sort_values(m, ascending=False)\n",
    "\n",
    "            if qs.empty:\n",
    "                continue\n",
    "\n",
    "            best_row = qs.iloc[0]\n",
    "            pred_section = str(best_row[\"section\"]).strip()\n",
    "            is_correct = pred_section in gold_list\n",
    "\n",
    "            records.append({\n",
    "                \"question_ind\": q_idx,\n",
    "                \"question\": question_text,\n",
    "                \"method\": m,\n",
    "                \"pred_section\": pred_section,\n",
    "                \"is_correct\": is_correct,\n",
    "                \"gold_sections\": gold_list\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "core_methods = [\"tfidf\", \"embedding\", \"attention_2_3\"]\n",
    "\n",
    "predictions_df = get_top_predictions(scores, questions, core_methods)\n",
    "predictions_df.head(10)\n",
    "\n",
    "output_path = r\"C:\\Users\\KVIRDI\\Downloads\\model_predictions.csv\"\n",
    "predictions_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved predictions to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa8cfdc",
   "metadata": {},
   "source": [
    "Model Predictions: Using the best attention model for a given question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffbd413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to: C:\\Users\\KVIRDI\\Downloads\\model_predictions_best_attention.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_ind</th>\n",
       "      <th>question</th>\n",
       "      <th>method</th>\n",
       "      <th>pred_section</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>gold_sections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What is the non-fixed radiation contamination ...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>2.4.6.2</td>\n",
       "      <td>True</td>\n",
       "      <td>[2.4.6.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>What is the non-fixed radiation contamination ...</td>\n",
       "      <td>embedding</td>\n",
       "      <td>2.4.6.4</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.4.6.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>What is the non-fixed radiation contamination ...</td>\n",
       "      <td>attention_2_1</td>\n",
       "      <td>Appendix 15</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.4.6.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>What procedures should be followed if the non-...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>2.4.14.2</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.4.6.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>What procedures should be followed if the non-...</td>\n",
       "      <td>embedding</td>\n",
       "      <td>2.4.6.4</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.4.6.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>What procedures should be followed if the non-...</td>\n",
       "      <td>attention_2_3</td>\n",
       "      <td>Appendix 15</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.4.6.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>When should the wipe tests be done after using...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>2.4.23</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.4.6.1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>When should the wipe tests be done after using...</td>\n",
       "      <td>embedding</td>\n",
       "      <td>2.4.6.4</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.4.6.1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>When should the wipe tests be done after using...</td>\n",
       "      <td>attention_1_3</td>\n",
       "      <td>Appendix 15</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.4.6.1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>When is leak testing of sealed sources or devi...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>2.3.8</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.4.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>When is leak testing of sealed sources or devi...</td>\n",
       "      <td>embedding</td>\n",
       "      <td>2.3.8</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.4.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>When is leak testing of sealed sources or devi...</td>\n",
       "      <td>attention_5_4</td>\n",
       "      <td>Appendix 15</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.4.8]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    question_ind                                           question  \\\n",
       "0              0  What is the non-fixed radiation contamination ...   \n",
       "1              0  What is the non-fixed radiation contamination ...   \n",
       "2              0  What is the non-fixed radiation contamination ...   \n",
       "3              1  What procedures should be followed if the non-...   \n",
       "4              1  What procedures should be followed if the non-...   \n",
       "5              1  What procedures should be followed if the non-...   \n",
       "6              2  When should the wipe tests be done after using...   \n",
       "7              2  When should the wipe tests be done after using...   \n",
       "8              2  When should the wipe tests be done after using...   \n",
       "9              3  When is leak testing of sealed sources or devi...   \n",
       "10             3  When is leak testing of sealed sources or devi...   \n",
       "11             3  When is leak testing of sealed sources or devi...   \n",
       "\n",
       "           method pred_section  is_correct gold_sections  \n",
       "0           tfidf      2.4.6.2        True     [2.4.6.2]  \n",
       "1       embedding      2.4.6.4       False     [2.4.6.2]  \n",
       "2   attention_2_1  Appendix 15       False     [2.4.6.2]  \n",
       "3           tfidf     2.4.14.2       False     [2.4.6.2]  \n",
       "4       embedding      2.4.6.4       False     [2.4.6.2]  \n",
       "5   attention_2_3  Appendix 15       False     [2.4.6.2]  \n",
       "6           tfidf       2.4.23       False     [2.4.6.1]  \n",
       "7       embedding      2.4.6.4       False     [2.4.6.1]  \n",
       "8   attention_1_3  Appendix 15       False     [2.4.6.1]  \n",
       "9           tfidf        2.3.8       False       [2.4.8]  \n",
       "10      embedding        2.3.8       False       [2.4.8]  \n",
       "11  attention_5_4  Appendix 15       False       [2.4.8]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_top_predictions_with_best_attention(scores_df, questions_df, baseline_methods):\n",
    "    \n",
    "    records = []\n",
    "    valid_q_indices = sorted(scores_df[\"question_ind\"].unique())\n",
    "\n",
    "    attention_cols = [c for c in scores_df.columns if c.startswith(\"attention_\")]\n",
    "\n",
    "    for q_idx in valid_q_indices:\n",
    "        if q_idx >= len(questions_df):\n",
    "            continue\n",
    "\n",
    "        gold_list = questions_df.loc[q_idx, \"gold_sections\"]\n",
    "        question_text = questions_df.loc[q_idx, \"question\"]\n",
    "\n",
    "        q_scores = scores_df[scores_df[\"question_ind\"] == q_idx]\n",
    "\n",
    "        for m in baseline_methods:\n",
    "            if m not in scores_df.columns:\n",
    "                continue\n",
    "\n",
    "            qs = q_scores.sort_values(m, ascending=False)\n",
    "            if qs.empty:\n",
    "                continue\n",
    "\n",
    "            best_row = qs.iloc[0]\n",
    "            pred_section = str(best_row[\"section\"]).strip()\n",
    "            is_correct = pred_section in gold_list\n",
    "\n",
    "            records.append({\n",
    "                \"question_ind\": q_idx,\n",
    "                \"question\": question_text,\n",
    "                \"method\": m,\n",
    "                \"pred_section\": pred_section,\n",
    "                \"is_correct\": is_correct,\n",
    "                \"gold_sections\": gold_list\n",
    "            })\n",
    "\n",
    "        if attention_cols:\n",
    "            best_head_name = None\n",
    "            best_head_score = None\n",
    "            best_head_section = None\n",
    "\n",
    "            for att in attention_cols:\n",
    "                qs_att = q_scores.sort_values(att, ascending=False)\n",
    "                if qs_att.empty:\n",
    "                    continue\n",
    "\n",
    "                top_row = qs_att.iloc[0]\n",
    "                top_score = top_row[att]\n",
    "                top_section = str(top_row[\"section\"]).strip()\n",
    "\n",
    "                if (best_head_score is None) or (top_score > best_head_score):\n",
    "                    best_head_score = top_score\n",
    "                    best_head_section = top_section\n",
    "                    best_head_name = att\n",
    "\n",
    "            if best_head_name is not None:\n",
    "                is_correct = best_head_section in gold_list\n",
    "\n",
    "                records.append({\n",
    "                    \"question_ind\": q_idx,\n",
    "                    \"question\": question_text,\n",
    "                    \"method\": best_head_name, \n",
    "                    \"pred_section\": best_head_section,\n",
    "                    \"is_correct\": is_correct,\n",
    "                    \"gold_sections\": gold_list\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "baseline_methods = [\"tfidf\", \"embedding\"]\n",
    "\n",
    "predictions_df = get_top_predictions_with_best_attention(scores, questions, baseline_methods)\n",
    "\n",
    "output_path = r\"C:\\Users\\KVIRDI\\Downloads\\model_predictions_best_attention.csv\"\n",
    "predictions_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved predictions to: {output_path}\")\n",
    "predictions_df.head(12)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
